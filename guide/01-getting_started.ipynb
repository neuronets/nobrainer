{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "01-getting_started.ipynb",
      "provenance": []
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZjOrKRnq5aa"
      },
      "source": [
        "# Getting started with Nobrainer\n",
        "\n",
        "Nobrainer is a deep learning framework for 3D image processing. It implements several 3D convolutional models from recent literature, methods for loading and augmenting volumetric data than can be used with any TensorFlow or Keras model, losses and metrics for 3D data, and utilities for model training, evaluation, prediction, and transfer learning.\n",
        "\n",
        "The code for the Nobrainer framework is available on GitHub at https://github.com/neuronets/nobrainer. The Nobrainer project is supported by NIH RF1MH121885 and previously by R01EB020470. It is distributed under the Apache 2.0 license.\n",
        "\n",
        "## Questions or issues\n",
        "\n",
        "If you have questions about Nobrainer or encounter any issues using the framework, please [submit a GitHub issue](https://github.com/neuronets/helpdesk/issues/new/choose). If you have a feature request, we encourage you to submit a pull request."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwaWX82pq5ad"
      },
      "source": [
        "# Using the guide notebooks\n",
        "\n",
        "Please install `nobrainer` before using these notebooks. You can learn how to do this below. Most of the notebooks also require some data on which to train or evaluate. You can use your own data, but `nobrainer` also provides a utility to download a small public dataset. Please refer to the notebook `02-preparing_training_data.ipynb` to download and prepare the example data for use or to prepare your own data for use.\n",
        "\n",
        "After you have gone through `02-preparing_training_data.ipynb`, please take a look at the other notebooks in this guide.\n",
        "\n",
        "## Google Colaboratory\n",
        "\n",
        "These notebooks can be [run for free](https://colab.research.google.com/github/neuronets/nobrainer) on Google Colaboratory (you must be signed into a Google account). If you are using Colab, please note that multiple open tabs of Colab notebooks will use the same resources (RAM, GPU). Downloading data in multiple Colab notebooks at the same time or training multiple models can quickly exhaust the available resources. For this reason, please run one notebook at a time, and keep an eye on the resources used.\n",
        "\n",
        "Users can choose to run Colab notebooks on CPU, GPU, or TPU. By default, the notebooks will use the CPU runtime. To use a different runtime, please select `Runtime > Change runtime type` in the menu bar. Then, choose either `GPU` or `TPU` under `Hardware accelerator`. No code changes are required when running on CPU or GPU runtime. When using the TPU runtime, however, special care must be taken for things to work properly. Please refer to the TPU guide notebook in this directory for more information.\n",
        "\n",
        "## Jupyter Notebook\n",
        "\n",
        "These notebooks can use whatever hardware you have available, whether it is CPU, GPU, or TPU. Please note that training models on CPU can take a very long time. GPUs will greatly increase speed of training and inference. Some of the notebooks download example data, but you can feel free to use your own data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fO5RSFbq5ad"
      },
      "source": [
        "# Install Nobrainer\n",
        "\n",
        "Nobrainer can be installed using `pip`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xELN1Hcq5ae"
      },
      "source": [
        "!pip install --no-cache-dir nobrainer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVMXci7Mq5ae"
      },
      "source": [
        "# Accessing Nobrainer\n",
        "\n",
        "## Command-line\n",
        "\n",
        "Nobrainer provides the command-line program `nobrainer`, which contains various methods for preparing data, training and evaluating models, generating predictions, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HU6edUxDq5af"
      },
      "source": [
        "!nobrainer --help"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AP6dWeNXq5af"
      },
      "source": [
        "## Python\n",
        "\n",
        "The `nobrainer` Python package can be imported as below. This gives you access to all of nobrainer's modules."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8mYOLaWq5af"
      },
      "source": [
        "import nobrainer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKpEQi0Xq5ag"
      },
      "source": [
        "### Layout\n",
        "\n",
        "- `nobrainer.io`: input/output methods\n",
        "- `nobrainer.layers`: custom Keras layers\n",
        "- `nobrainer.losses`: loss functions for volumetric segmentation\n",
        "- `nobrainer.metrics`: metrics for volumetric segmentation\n",
        "- `nobrainer.models`: pre-defined Keras models\n",
        "- `nobrainer.tfrecords`: writing and reading of TFRecords files\n",
        "- `nobrainer.transform`: rigid transformations for data augmentation\n",
        "- `nobrainer.volume`: `tf.data.Dataset` creation and data augmentation utilities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VG0lFj2gq5ag"
      },
      "source": [
        "# Next steps\n",
        "\n",
        "Here are other Jupyter notebooks in this guide to learn how to prepare your training data, train models on different hardware (single GPU, multiple GPUs, or TPU), and more. These tutorial notebooks will be updated and enhanced regularly. If you think something is missing or could be improved, please [submit a GitHub issue](https://github.com/neuronets/helpdesk/issues/new/choose).\n",
        "\n",
        "## Tutorials:\n",
        "\n",
        "### Training examples\n",
        "\n",
        "- [Preparing training data](https://colab.research.google.com/github/neuronets/nobrainer/blob/master/guide/02-preparing_training_data.ipynb)\n",
        "- [Train binary classification](https://colab.research.google.com/github/neuronets/nobrainer/blob/master/guide/train_binary_classification.ipynb)\n",
        "- [Train binary segmentation](https://colab.research.google.com/github/neuronets/nobrainer/blob/master/guide/train_binary_segmentation.ipynb)\n",
        "- [Train on multiple GPUs](https://colab.research.google.com/github/neuronets/nobrainer/blob/master/guide/train_on_multiple_gpus.ipynb)\n",
        "- [Train/Use a progressive GAN](https://colab.research.google.com/github/neuronets/nobrainer/blob/master/guide/train_generation_progressive.ipynb)\n",
        "- [Transfer learning example](https://colab.research.google.com/github/neuronets/nobrainer/blob/master/guide/transfer_learning.ipynb)\n",
        "\n",
        "### Inference examples\n",
        "\n",
        "- [Inference using kwyk](https://colab.research.google.com/github/neuronets/nobrainer/blob/master/guide/Inference_with_kwyk_model.ipynb)",
        "- [Train/Use a progressive GAN](https://colab.research.google.com/github/neuronets/nobrainer/blob/master/guide/train_generation_progressive.ipynb)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nf6LQLwcob5i"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
