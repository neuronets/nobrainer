{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "train_generation_progressive.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlHhhplL7opm"
      },
      "source": [
        "# Train a neural network for mri generation using progressive growing\n",
        "\n",
        "In this notebook, we will use Nobrainer to train a model for brain MRI generation. Brain MRI generation is a useful task in synthetically creating neuroimaging data. We will use a Generative Adversarial Network to model the generation and use a progressive growing training method for high quality generation at higher resolutions.\n",
        "\n",
        "In the following cells, we will:\n",
        "\n",
        "1. Get sample T1-weighted MR scans as features.\n",
        "2. Convert the data to TFRecords format.\n",
        "3. Instantiate a progressive convolutional neural network for generator and discriminator.\n",
        "4. Create a Dataset of the features.\n",
        "5. Instantiate a trainer and choose a loss function to use.\n",
        "6. Train on part of the data in two phases (transition and resolution).\n",
        "7. Repeat steps 4-6 for each growing resolution.\n",
        "8. Generate some images using trained model\n",
        "\n",
        "## Google Colaboratory\n",
        "\n",
        "If you are using Colab, please switch your runtime to GPU. To do this, select `Runtime > Change runtime type` in the top menu. Then select GPU under `Hardware accelerator`. A GPU greatly speeds up training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUwl5vYH7rrD"
      },
      "source": [
        "!pip install --no-cache-dir nobrainer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qpm8u9O47opq"
      },
      "source": [
        "import nobrainer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAl3sk8e7opr"
      },
      "source": [
        "# Get sample features and labels\n",
        "\n",
        "We use 9 pairs of volumes for training and 1 pair of volumes for evaluation. Many more volumes would be required to train a model for any useful purpose."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yV9F64HE7opr"
      },
      "source": [
        "csv_of_filepaths = nobrainer.utils.get_data()\n",
        "filepaths = nobrainer.io.read_csv(csv_of_filepaths)\n",
        "\n",
        "train_paths = filepaths[:9]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqzlAmyI7ops"
      },
      "source": [
        "# Convert medical images to TFRecords\n",
        "\n",
        "Remember how many full volumes are in the TFRecords files. This will be necessary to know how many steps are in on training epoch. The default training method needs to know this number, because Datasets don't always know how many items they contain."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PvvTXmO7ops"
      },
      "source": [
        "!mkdir -p data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFJ1Yl9y7opt"
      },
      "source": [
        "resolution_batch_size_map = {8: 1, 16: 1, 32: 1, 64: 1, 128: 1, 256: 1} \n",
        "# resolution_batch_size_map = {8: 32, 16: 16, 32: 8, 64: 4, 128: 1, 256: 1} uncomment when sufficient compute is available\n",
        "resolutions = sorted(list(resolution_batch_size_map.keys()))\n",
        "\n",
        "nobrainer.tfrecord.write(\n",
        "    features_labels=train_paths,\n",
        "    filename_template='data/data-train_shard-{shard:03d}.tfrec',\n",
        "    examples_per_shard=3, # change for larger dataset\n",
        "    multi_resolution=True,\n",
        "    resolutions=resolutions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N58H-gun7opt"
      },
      "source": [
        "# Set the Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6yR0Cq37opu"
      },
      "source": [
        "latent_size = 256\n",
        "g_fmap_base = 1024\n",
        "d_fmap_base = 1024\n",
        "# latent_size = 1024 uncomment when sufficient compute is available\n",
        "# g_fmap_base = 4096 uncomment when sufficient compute is available\n",
        "# d_fmap_base = 4096 uncomment when sufficient compute is available\n",
        "num_parallel_calls = 4\n",
        "iterations = int(10)\n",
        "# iterations = int(300e3) uncomment when sufficient compute is available\n",
        "lr = 1e-4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CUSYF427opu"
      },
      "source": [
        "# Create logging directories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghqyOT587opv"
      },
      "source": [
        "from pathlib import Path\n",
        "\n",
        "save_dir = 'temp'\n",
        "\n",
        "save_dir = Path(save_dir)\n",
        "generated_dir = save_dir.joinpath('generated')\n",
        "model_dir = save_dir.joinpath('saved_models')\n",
        "log_dir = save_dir.joinpath('logs')\n",
        "\n",
        "save_dir.mkdir(exist_ok=True)\n",
        "generated_dir.mkdir(exist_ok=True)\n",
        "model_dir.mkdir(exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJu8zPdr7opv"
      },
      "source": [
        "# Instantiate a neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iA2uWJUa7opv"
      },
      "source": [
        "generator, discriminator = nobrainer.models.progressivegan(latent_size, \n",
        "                                                           g_fmap_base=g_fmap_base, \n",
        "                                                           d_fmap_base=d_fmap_base)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bl7FeQGm7opw"
      },
      "source": [
        "# Train the network progressively for each resolution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HemLOnB9CAum"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvdWJq6p7opw"
      },
      "source": [
        "for resolution in resolutions:\n",
        "    \n",
        "    # create a train dataset with features for resolution\n",
        "    dataset_train = nobrainer.dataset.get_dataset(\n",
        "        file_pattern=\"data/*res-%03d*.tfrec\"%(resolution),\n",
        "        batch_size=resolution_batch_size_map[resolution],\n",
        "        num_parallel_calls=num_parallel_calls,\n",
        "        volume_shape=(resolution, resolution, resolution),\n",
        "        n_classes=1, # dummy labels as this is unsupervised training\n",
        "        scalar_label=True,\n",
        "        standardize=False\n",
        "    )\n",
        "\n",
        "    # grow the networks by one (2^x) resolution\n",
        "    generator.add_resolution()\n",
        "    discriminator.add_resolution()\n",
        "\n",
        "    # instantiate a progressive training helper\n",
        "    progressive_gan_trainer = nobrainer.training.ProgressiveGANTrainer(\n",
        "        generator=generator,\n",
        "        discriminator=discriminator,\n",
        "        gradient_penalty=True)\n",
        "\n",
        "    # compile with optimizers and loss function of choice\n",
        "    progressive_gan_trainer.compile(\n",
        "        g_optimizer=tf.keras.optimizers.Adam(learning_rate=lr, beta_1=0.0, beta_2=0.99, epsilon=1e-8),\n",
        "        d_optimizer=tf.keras.optimizers.Adam(learning_rate=lr, beta_1=0.0, beta_2=0.99, epsilon=1e-8),\n",
        "        g_loss_fn=nobrainer.losses.wasserstein,\n",
        "        d_loss_fn=nobrainer.losses.wasserstein\n",
        "        )\n",
        "\n",
        "    steps_per_epoch = iterations//resolution_batch_size_map[resolution]\n",
        "    # save_best_only is set to False as it is an adversarial loss\n",
        "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(str(model_dir), save_weights_only=True, save_best_only=False, save_freq=10)\n",
        "\n",
        "    # Train at resolution\n",
        "    print('Resolution : {}'.format(resolution))\n",
        "\n",
        "    print('Transition phase')\n",
        "    progressive_gan_trainer.fit(\n",
        "        dataset_train,\n",
        "        phase='transition',\n",
        "        resolution=resolution,\n",
        "        steps_per_epoch=steps_per_epoch, # necessary for repeat dataset\n",
        "        callbacks=[model_checkpoint_callback])\n",
        "\n",
        "    print('Resolution phase')\n",
        "    progressive_gan_trainer.fit(\n",
        "        dataset_train,\n",
        "        phase='resolution',\n",
        "        resolution=resolution,\n",
        "        steps_per_epoch=steps_per_epoch,\n",
        "        callbacks=[model_checkpoint_callback])\n",
        "\n",
        "    # save the final weights\n",
        "    generator.save(str(model_dir.joinpath('generator_final_res_{}'.format(resolution))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8mhgYRfB-3R"
      },
      "source": [
        "progressive_gan_trainer.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czNPZfBhCjZJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
