{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jlHhhplL7opm",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Train a neural network for mri generation using progressive growing\n",
    "\n",
    "In this notebook, we will use Nobrainer to train a model for brain MRI generation. Brain MRI generation is a useful task in synthetically creating neuroimaging data. We will use a Generative Adversarial Network to model the generation and use a progressive growing training method for high quality generation at higher resolutions.\n",
    "\n",
    "In the following cells, we will:\n",
    "\n",
    "1. Get sample T1-weighted MR scans as features.\n",
    "2. Convert the data to TFRecords format.\n",
    "3. Instantiate a progressive convolutional neural network for generator and discriminator.\n",
    "4. Create a Dataset of the features.\n",
    "5. Instantiate a trainer and choose a loss function to use.\n",
    "6. Train on part of the data in two phases (transition and resolution).\n",
    "7. Repeat steps 4-6 for each growing resolution.\n",
    "8. Generate some images using trained model\n",
    "\n",
    "## Google Colaboratory\n",
    "\n",
    "If you are using Colab, please switch your runtime to GPU. To do this, select `Runtime > Change runtime type` in the top menu. Then select GPU under `Hardware accelerator`. A GPU greatly speeds up training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sUwl5vYH7rrD",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install --no-cache-dir nilearn nobrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qpm8u9O47opq",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import nobrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QAl3sk8e7opr",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Get sample features and labels\n",
    "\n",
    "We use 9 pairs of volumes for training and 1 pair of volumes for evaluation. Many more volumes would be required to train a model for any useful purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yV9F64HE7opr",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "csv_of_filepaths = nobrainer.utils.get_data()\n",
    "filepaths = nobrainer.io.read_csv(csv_of_filepaths)\n",
    "\n",
    "train_paths = filepaths[:9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jqzlAmyI7ops",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Convert medical images to TFRecords\n",
    "\n",
    "Remember how many full volumes are in the TFRecords files. This will be necessary to know how many steps are in on training epoch. The default training method needs to know this number, because Datasets don't always know how many items they contain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from nobrainer.dataset import write_multi_resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "datasets = write_multi_resolution(train_paths, \n",
    "                                  tfrecdir=\"data/generate\",\n",
    "                                  n_processes=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The datasets will look like the following. One can adjust the `batch size` depending on compute power, but also epochs and normalizers.\n",
    "\n",
    "```python\n",
    "datasets = {8: {'file_pattern': 'data/generate/*res-008.tfrec',\n",
    "  'batch_size': 1,\n",
    "  'normalizer': None},\n",
    " 16: {'file_pattern': 'data/generate/*res-016.tfrec',\n",
    "  'batch_size': 1,\n",
    "  'normalizer': None},\n",
    " 32: {'file_pattern': 'data/generate/*res-032.tfrec',\n",
    "  'batch_size': 1,\n",
    "  'normalizer': None},\n",
    " 64: {'file_pattern': 'data/generate/*res-064.tfrec',\n",
    "  'batch_size': 1,\n",
    "  'normalizer': None},\n",
    " 128: {'file_pattern': 'data/generate/*res-128.tfrec',\n",
    "  'batch_size': 1,\n",
    "  'normalizer': None},\n",
    " 256: {'file_pattern': 'data/generate/*res-256.tfrec',\n",
    "  'batch_size': 1,\n",
    "  'normalizer': None}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from nobrainer.volume import normalize, adjust_dynamic_range\n",
    "\n",
    "def scale(x):\n",
    "    \"\"\"Scale data to -1 to 1\"\"\"\n",
    "    return adjust_dynamic_range(normalize(x), [0, 1], [-1, 1])\n",
    "\n",
    "# datasets[16][\"normalizer\"] = scale\n",
    "\n",
    "# Adjust number of epochs\n",
    "datasets[8][\"epochs\"] = 1000\n",
    "datasets[16][\"epochs\"] = 1000\n",
    "datasets[32][\"epochs\"] = 400\n",
    "datasets[64][\"epochs\"] = 200\n",
    "\n",
    "# Adjust batch size from the default of 1\n",
    "datasets[8][\"batch_size\"] = 8\n",
    "datasets[16][\"batch_size\"] = 8\n",
    "datasets[32][\"batch_size\"] = 8\n",
    "datasets[64][\"batch_size\"] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from nobrainer.processing.generation import ProgressiveGeneration\n",
    "gen = ProgressiveGeneration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# epochs and normalizer can be overwritten by resolution specific settings in datasets\n",
    "gen.fit(datasets, \n",
    "        epochs=10, \n",
    "        normalizer=scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Scale output to uint16\n",
    "\n",
    "One can return the native datatype by not passing a `data_type` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from nilearn import plotting\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "images = gen.generate(data_type=np.uint8, n_images=10)\n",
    "\n",
    "fig, ax = plt.subplots(len(images), 1, figsize=(18, 30))\n",
    "index = 0 \n",
    "for img in images:\n",
    "    plotting.plot_anat(anat_img=img, figure=fig, axes=ax[index], \n",
    "                       draw_cross=False)\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Warm restart at the last resolution\n",
    "\n",
    "We can warm start the training, but for the moment it will only retrain using the final resolution of the data or higher. This can be used to:\n",
    "- split different resolution phases\n",
    "- use different datasets for different resolutions\n",
    "- fine tune on a specific dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gen.fit(datasets, warm_start=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gen.save(\"data/saved_gen_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### TODO: Load and reuse model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "train_generation_progressive.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
