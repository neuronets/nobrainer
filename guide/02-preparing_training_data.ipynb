{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing training data\n",
    "\n",
    "In this tutorial, we will convert medical imaging data to the TFRecords format. Having data in the TFRecords format speeds up training and allows us to use standard, highly-optimized TensorFlow I/O methods. We will then create a `tf.data.Dataset` object using the TFRecords data. This `tf.data.Dataset` object can be used for training, evaluation, or prediction.\n",
    "\n",
    "This tutorial will use publicly available data. To convert your own data, you will need to create a nested list of features and labels volumes. One can store this as a CSV that looks like the following:\n",
    "\n",
    "```\n",
    "features,labels\n",
    "/path/to/1_features.nii.gz,/path/to/1_labels.nii.gz\n",
    "/path/to/2_features.nii.gz,/path/to/2_labels.nii.gz\n",
    "/path/to/3_features.nii.gz,/path/to/3_labels.nii.gz\n",
    "/path/to/4_features.nii.gz,/path/to/4_labels.nii.gz\n",
    "```\n",
    "\n",
    "\n",
    "You can read this CSV in Python with `nobrainer.io.read_csv`.\n",
    "\n",
    "## Google Colaboratory\n",
    "\n",
    "If you are using Colab, please switch your runtime to GPU. To do this, select `Runtime > Change runtime type` in the top menu. Then select GPU under `Hardware accelerator`. A GPU is not necessary to prepare the data, but a GPU is helpful for training a model, which we demonstrate at the end of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nobrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get sample data\n",
    "\n",
    "Here, we download 10 T1-weighted brain scans and their corresponding FreeSurfer segmentations. These volumes take up about 46 MB and are saved to a temporary directory. The object `csv_path` is the path to a CSV file. Each row of this CSV file contains the paths to a pair of features and labels volumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = nobrainer.utils.get_data()\n",
    "\n",
    "!head $csv_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to volume files to TFRecords\n",
    "\n",
    "To achieve the best performance, training data should be in TFRecords format. This is the preferred file format for TensorFlow, Training can be done on medical imaging volume files but will be slower.\n",
    "\n",
    "Nobrainer has a command-line utility to convert volumes to TFRecords: `nobrainer convert`. This will verify that all of the volumes have the same shape and that the label volumes are an integer type or can be safely coerced to an integer type. \n",
    "\n",
    "Following successful verification, the volumes will be converted to TFRecords files. The dataset should be sharded into multiple TFRecords files, so that data can be shuffled more properly. This is especially helpful for large datasets. Users can choose how many pairs of volumes (i.e., features and labels) will be saved to one TFRecords file. In this example, we will save 3 pairs of volumes per TFRecords file because our dataset is small. With a larger dataset, users should choose a larger shard value. For example, with 10,000 volumes, one might choose 100 volumes per TFRecords file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!nobrainer convert --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p tfrecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nobrainer convert \\\n",
    "    --csv='/tmp/nobrainer-data/filepaths.csv' \\\n",
    "    --tfrecords-template='tfrecords/data_shard-{shard:03d}.tfrec' \\\n",
    "    --volumes-per-shard=2 \\\n",
    "    --volume-shape 256 256 256 \\\n",
    "    --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create input data pipeline\n",
    "\n",
    "We will now create an data pipeline to feed our models with training data. The steps below will create a `tensorflow.data.Dataset` object that is built according to [TensorFlow's guidelines](https://www.tensorflow.org/guide/performance/datasets). The basic pipeline is summarized below.\n",
    "\n",
    "- Read data\n",
    "- Separate volumes into non-overlapping sub-volumes\n",
    "    - This is done to get around memory limitations with larger models.\n",
    "    - For example, a volume with shape (256, 256, 256) can be separated into eight non-overlapping blocks of shape (128, 128, 128).\n",
    "- Apply random rigid augmentations if requested.\n",
    "- Standard score volumes of features.\n",
    "- Binarize labels if binary segmentation.\n",
    "- Replace values according to some mapping if multi-class segmentation.\n",
    "- Batch the results so every iteration yields `batch_size` elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A glob pattern to match the files we want to train on.\n",
    "file_pattern = 'tfrecords/data_shard-*.tfrec'\n",
    "\n",
    "# The number of classes the model predicts. A value of 1 means the model performs\n",
    "# binary classification (i.e., target vs background).\n",
    "n_classes = 1\n",
    "\n",
    "# Batch size is the number of features and labels we train on with each step.\n",
    "batch_size = 2\n",
    "\n",
    "# The shape of the original volumes.\n",
    "volume_shape = (256, 256, 256)\n",
    "\n",
    "# The shape of the non-overlapping sub-volumes. Most models cannot be trained on\n",
    "# full volumes because of hardware and memory constraints, so we train and evaluate\n",
    "# on sub-volumes.\n",
    "block_shape = (128, 128, 128)\n",
    "\n",
    "# Whether or not to apply random rigid transformations to the data on the fly.\n",
    "# This can improve model generalizability but increases processing time.\n",
    "augment = False\n",
    "\n",
    "# The tfrecords filepaths will be shuffled before reading, but we can also shuffle\n",
    "# the data. This will shuffle 10 volumes at a time. Larger buffer sizes will require\n",
    "# more memory, so choose a value based on how much memory you have available.\n",
    "shuffle_buffer_size = 10\n",
    "\n",
    "# Number of parallel processes to use.\n",
    "num_parallel_calls = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ls $file_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset = nobrainer.volume.get_dataset(\n",
    "    file_pattern=file_pattern,\n",
    "    n_classes=n_classes,\n",
    "    batch_size=batch_size,\n",
    "    volume_shape=volume_shape,\n",
    "    block_shape=block_shape,\n",
    "    augment=augment,\n",
    "    n_epochs=1,\n",
    "    shuffle_buffer_size=shuffle_buffer_size,\n",
    "    num_parallel_calls=num_parallel_calls)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a model\n",
    "\n",
    "We will briefly demonstrate how to train a model given the `tf.data.Dataset` we created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate a pre-defined `nobrainer` model\n",
    "\n",
    "Users can find pre-defined models under the namespace `nobrainer.models`. All models are implemented using the `tf.keras` API, which makes definitions highly readable and hackable, despite being a high-level interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = nobrainer.models.unet(n_classes=n_classes, input_shape=(*block_shape, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the model\n",
    "\n",
    "All Keras models must be compiled before they can be trained. This is where you choose the optimizer, loss function, and any metrics that should be reported during training. Nobrainer implements several loss functions useful for semantic segmentation, including Dice, Generalized Dice, Focal, Jaccard, and Tversky losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(lr=1e-04),\n",
    "    loss=nobrainer.losses.jaccard,\n",
    "    metrics=[nobrainer.metrics.dice])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on a single GPU\n",
    "\n",
    "To learn how to train on multiple GPUs or on a TPU, please refer to the other notebooks in the nobrainer guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = nobrainer.volume.get_steps_per_epoch(\n",
    "    n_volumes=10, \n",
    "    volume_shape=volume_shape, \n",
    "    block_shape=block_shape, \n",
    "    batch_size=batch_size)\n",
    "\n",
    "steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(dataset, steps_per_epoch=steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nb]",
   "language": "python",
   "name": "conda-env-nb-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
